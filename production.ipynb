{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <h1 align=\"center\">CheXRay: Automatically Diagnosing Chest X-Rays using Generated Radiologist Reports and Patient Information </h1>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\"\"\"\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=12gqE8PZUn6aE0akNpE_dd-xI_bp3LZ20\"\n",
    "output = 'models/sum.0.0.pth'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\"\"\"\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1Px0cAXOhSfoWoFwy1s23v43C5O1fhY9w\"\n",
    "output = 'models/sum_test.0.0.pth'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1yS4XJzEI_lIGOraFMoMtqhcCAMxnQzT1\"\n",
    "output = 'models/repgen.0.0.pth'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1Pzhd5qdXYWX7zNYBidO-WHKT0CJGJF1H\"\n",
    "output = 'models/txtcls.pkl'\n",
    "gdown.download(url, output, quiet=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q self-supervised\n",
    "%pip install -q transformers\n",
    "%pip install -q grad-cam\n",
    "%pip install -q opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Modules for helper functions\n",
    "from modules.utils.dicom import PILDicom2 #Because PILDicom from fastai doesn't work\n",
    "    \n",
    "#Fast.ai modules\n",
    "#!pip install -q pydicom pyarrow kornia opencv-python scikit-image nbdev\n",
    "from fastai.data.core import Datasets, DataLoaders, TfmdLists\n",
    "from fastai.data.block import MultiCategoryBlock\n",
    "from fastai.torch_core import TensorImage, to_np, Module\n",
    "from fastai.text.core import BaseTokenizer\n",
    "from fastai.tabular.core import make_date, cont_cat_split, Categorify, FillMissing, TabularPandas\n",
    "\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.data.transforms import IntToFloatTensor, Normalize, EncodedMultiCategorize, ToTensor\n",
    "from fastcore.foundation import L\n",
    "from fastcore.transform import Transform\n",
    "from fastai.text.data import Numericalize, pad_input, SortedDL, LMDataLoader\n",
    "\n",
    "from fastai.text.models.awdlstm import AWD_LSTM\n",
    "from fastai.learner import Learner\n",
    "from fastai.tabular.learner import tabular_learner\n",
    "from fastai.text.learner import text_classifier_learner\n",
    "\n",
    "#Modules for R2Gen/multimodal\n",
    "from modules.repgen.dataset import RepGenDataset\n",
    "from modules.repgen.dataloader import create_batch\n",
    "from modules.repgen.model import R2GenModel\n",
    "from modules.repgen.loss import compute_loss\n",
    "from modules.repgen.fastai_utils import rep_gen, SelectPred\n",
    "from modules.repgen.metrics import bleu4\n",
    "\n",
    "#Modules for sum\n",
    "from modules.sum.dataloader import SumDL  \n",
    "import modules.sum.logits as log1\n",
    "from modules.sum.model import SumModel\n",
    "from modules.sum.loss import SumGradientBlending\n",
    "from modules.sum.fastai_utils import sum_splitter\n",
    "from modules.sum.metrics import ap_weighted\n",
    "\n",
    "from self_supervised.models.vision_transformer import deit_small\n",
    "from self_supervised.layers import create_cls_module\n",
    "from fastai.optimizer import ranger\n",
    "from modules.vis.loss import MultiLabelSmoothingLoss\n",
    "\n",
    "from torchvision import transforms as pth_transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "\n",
    "#Other libraries\n",
    "import re\n",
    "import gc\n",
    "import cv2\n",
    "import html\n",
    "import torch\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pylab as plt\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from ipywidgets import VBox,widgets,Button,Layout,Box,Output,Label,FileUpload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageToolPublicAPI('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Path object which contains path to data\n",
    "prep = Path('./data/')\n",
    "prod_path = Path('./sample/')\n",
    "classes=[\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Enlarged_Cardiomediastinum\", \"Fracture\", \n",
    "         \"Lung_Lesion\", \"Lung_Opacity\", \"No_Finding\", \"Pleural_Effusion\", \"Pleural_Other\", \"Pneumonia\", \n",
    "         \"Pneumothorax\", \"Support_Devices\"]\n",
    "views = ['AP','AP_AXIAL','AP_LLD','AP_RLD','PA','PA_LLD','PA_RLD','LATERAL','LL','LAO','RAO','SWIMMERS','XTABLE_LATERAL','LPO']\n",
    "number_views = [144818,\n",
    "                    2,\n",
    "                    2,\n",
    "                    2,\n",
    "                    95145,\n",
    "                    2058,\n",
    "                    339,\n",
    "                    81939,\n",
    "                    42371,\n",
    "                    5188,\n",
    "                    3,\n",
    "                    13,\n",
    "                    2,\n",
    "                    1]\n",
    "workers = multiprocessing.cpu_count()\n",
    "\n",
    "defaults = SimpleNamespace(cpus=workers, cmap='viridis', return_fig=False, silent=False)\n",
    "defaults.device = torch.device('cpu') \n",
    "cpu = torch.device(\"cpu\")\n",
    "beta=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = widgets.HTML(value='<style>p{word-wrap: break-word}</style><p>')\n",
    "heading.value += \"This program takes the patient's chest x-ray(s), formatted as .dcm files, as input and<br/>\"\n",
    "heading.value += \"1) writes a radiologist report using the chest x-ray(s),<br/>\"\n",
    "heading.value += \"2) creates tabular data using the time and date,<br/>\"\n",
    "heading.value += \"3) and generates a summary of the diagnosis alongside heatmap and intrinsic attention visualizations for model interpretability.</p>\"\n",
    "heading.value += \"Notes:<br/>\" \n",
    "heading.value += \"- To upload multiple instances of a view, select all of the instances and upload them all at once.<br/>\"\n",
    "heading.value += \"- Although the x-rays are saved as files in this website, they remain within your environment and are deleted after the website closes.<br/>\"\n",
    "heading.value += \"- You can expect the program to complete within 3-6 minutes.<br/>\"\n",
    "heading.value += \"- Using approximately 8% of the MIMIC-CXR dataset for training, when evaluated on the official test set, the report generation model achieves a Bleu4 score of 0.0704 while the diagnosis model achieves a PR-AUC score of .7688, 54.49% precision, 82.41% recall, and an F\" + str(beta) + \" score of .733. A state-of-the-art model to compare these metrics to can be found at https://aclanthology.org/2020.emnlp-main.112.pdf.<br/>\"\n",
    "heading.value += \"- Metrics such as accuracy and ROC AUC were avoided in the optimization of the model and in their reporting because of the dataset's imbalance between positive and negative examples for each condition.<br/>\"\n",
    "heading.value += \"- If you have any questions or concerns, please contact the author at the following email: ajhinh@gmail.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_direct = widgets.Label()\n",
    "ap_direct.value = \"If (an) AP view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_axial_direct = widgets.Label()\n",
    "ap_axial_direct.value = \"If (an) AP axial view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_axial_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_lld_direct = widgets.Label()\n",
    "ap_lld_direct.value = \"If (an) AP LLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_lld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_rld_direct = widgets.Label()\n",
    "ap_rld_direct.value = \"If (an) AP RLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_rld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_direct = widgets.Label()\n",
    "pa_direct.value = \"If (a) PA view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_lld_direct = widgets.Label()\n",
    "pa_lld_direct.value = \"If (a) PA LLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_lld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_rld_direct = widgets.Label()\n",
    "pa_rld_direct.value = \"If (a) PA RLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_rld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_direct = widgets.Label()\n",
    "lat_direct.value = \"If (a) lateral view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_direct = widgets.Label()\n",
    "ll_direct.value = \"If (a) LL view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lao_direct = widgets.Label()\n",
    "lao_direct.value = \"If (a) LAO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lao_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rao_direct = widgets.Label()\n",
    "rao_direct.value = \"If (a) RAO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rao_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_direct = widgets.Label()\n",
    "swim_direct.value = \"If (a) swimmers view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab_lat_direct = widgets.Label()\n",
    "xtab_lat_direct.value = \"If (a) xtable lateral view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab_lat_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo_direct = widgets.Label()\n",
    "lpo_direct.value = \"If (a) LPO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_report = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cond = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sumvis = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = widgets.HTML(value='<style>p{word-wrap: break-word}</style><p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pl = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose = widgets.Button(description='Diagnose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_classify(change):    \n",
    "    heading.close()\n",
    "    ap_direct.close()\n",
    "    ap_btn_upload.close()\n",
    "    ap_axial_direct.close()\n",
    "    ap_axial_btn_upload.close()\n",
    "    ap_lld_direct.close()\n",
    "    ap_lld_btn_upload.close()\n",
    "    ap_rld_direct.close()\n",
    "    ap_rld_btn_upload.close()\n",
    "    pa_direct.close()\n",
    "    pa_btn_upload.close()\n",
    "    pa_lld_direct.close()\n",
    "    pa_lld_btn_upload.close()\n",
    "    pa_rld_direct.close()\n",
    "    pa_rld_btn_upload.close()\n",
    "    lat_direct.close()\n",
    "    lat_btn_upload.close()\n",
    "    ll_direct.close()\n",
    "    ll_btn_upload.close()\n",
    "    lao_direct.close()\n",
    "    lao_btn_upload.close()\n",
    "    rao_direct.close()\n",
    "    rao_btn_upload.close()\n",
    "    swim_direct.close()\n",
    "    swim_btn_upload.close()\n",
    "    xtab_lat_direct.close()\n",
    "    xtab_lat_btn_upload.close()\n",
    "    lpo_direct.close()\n",
    "    lpo_btn_upload.close()\n",
    "    diagnose.close()\n",
    "    \n",
    "    gen_report.value += \"Writing Report...\"\n",
    "    \n",
    "    input_views = []\n",
    "    input_paths = []\n",
    "\n",
    "    if ap_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP\")\n",
    "        for path in range(len(ap_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_btn_upload.value[list(ap_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_axial_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_AXIAL\")\n",
    "        for path in range(len(ap_axial_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_AXIAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_axial_btn_upload.value[list(ap_axial_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_lld_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_LLD\")\n",
    "        for path in range(len(ap_lld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_LLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_lld_btn_upload.value[list(ap_lld_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_rld_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_RLD\")\n",
    "        for path in range(len(ap_rld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_RLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_rld_btn_upload.value[list(ap_rld_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA\")\n",
    "        for path in range(len(pa_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_btn_upload.value[list(pa_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_lld_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA_LLD\")\n",
    "        for path in range(len(pa_lld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_LLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_lld_btn_upload.value[list(pa_lld_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_rld_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA_RLD\")\n",
    "        for path in range(len(pa_rld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_RLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_rld_btn_upload.value[list(pa_rld_btn_upload.value.keys())[0]]['content'])\n",
    "    if lat_btn_upload.data!=[]:\n",
    "        input_views.append(\"LATERAL\")\n",
    "        for path in range(len(lat_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LATERAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lat_btn_upload.value[list(lat_btn_upload.value.keys())[0]]['content'])\n",
    "    if ll_btn_upload.data!=[]:\n",
    "        input_views.append(\"LL\")\n",
    "        for path in range(len(ll_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ll_btn_upload.value[list(ll_btn_upload.value.keys())[0]]['content'])\n",
    "    if lao_btn_upload.data!=[]:\n",
    "        input_views.append(\"LAO\")\n",
    "        for path in range(len(lao_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LAO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lao_btn_upload.value[list(lao_btn_upload.value.keys())[0]]['content'])\n",
    "    if rao_btn_upload.data!=[]:\n",
    "        input_views.append(\"RAO\")\n",
    "        for path in range(len(rao_btn_upload.data)):\n",
    "            temp_path = prod_path/str('RAO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(rao_btn_upload.value[list(rao_btn_upload.value.keys())[0]]['content'])\n",
    "    if swim_btn_upload.data!=[]:\n",
    "        input_views.append(\"SWIMMERS\")\n",
    "        for path in range(len(swim_btn_upload.data)):\n",
    "            temp_path = prod_path/str('SWIMMERS_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(swim_btn_upload.value[list(swim_btn_upload.value.keys())[0]]['content'])\n",
    "    if xtab_lat_btn_upload.data!=[]:\n",
    "        input_views.append(\"XTABLE_LATERAL\")\n",
    "        for path in range(len(xtab_lat_btn_upload.data)):\n",
    "            temp_path = prod_path/str('XTABLE_LATERAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(xtab_lat_btn_upload.value[list(xtab_lat_btn_upload.value.keys())[0]]['content'])\n",
    "    if lpo_btn_upload.data!=[]:\n",
    "        input_views.append(\"LPO\")\n",
    "        for path in range(len(lpo_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LPO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lpo_btn_upload.value[list(lpo_btn_upload.value.keys())[0]]['content'])\n",
    "    \n",
    "    single_repgen_trainval_sample_path = prep/'trainval_sample_repgen_nomiss.csv'\n",
    "    vocab_path = Path('modules/repgen/vocab.pkl')\n",
    "    trainval_sample_single = pd.read_csv(single_repgen_trainval_sample_path)\n",
    "    trainval_sample_single['images']=prod_path/\"AP_0.dcm\"\n",
    "    trainval_sample_single.drop([10728, 10729], inplace=True)\n",
    "    train_sample_single = trainval_sample_single[trainval_sample_single['split']==False]\n",
    "    val_sample_single = trainval_sample_single[trainval_sample_single['split']==True]\n",
    "    train_sample_single.reset_index(drop=True, inplace=True)\n",
    "    val_sample_single.reset_index(drop=True, inplace=True)\n",
    "    with open(vocab_path, 'rb') as f: vocab = pickle.load(f)    \n",
    "        \n",
    "    df = trainval_sample_single.iloc[:len(input_paths)].copy()\n",
    "    df['images'] = input_paths\n",
    "\n",
    "    isval=False\n",
    "    viewtype='images' \n",
    "    ispred=False\n",
    "    train_sample_dataset = RepGenDataset(train_sample_single,isval, viewtype, ispred, classes) \n",
    "    isval=True\n",
    "    val_sample_dataset = RepGenDataset(val_sample_single,isval, viewtype, ispred, classes) \n",
    "    bs=16\n",
    "    trainval_sample_dls = DataLoaders.from_dsets(train_sample_dataset, val_sample_dataset, bs=bs, device=cpu, create_batch=create_batch, num_workers=workers, shuffle=True)\n",
    "    trainval_sample_dls.valid = trainval_sample_dls.valid.new(shuffle=False)\n",
    "\n",
    "    # Model settings (for visual extractor)\n",
    "    visual_extractor='resnet50' #'resnet101'\n",
    "    pretrained=True\n",
    "    # Model settings (for Transformer)  \n",
    "    num_layers=3 #number of layers of Transformer\n",
    "    d_model=512 #dimension of Transformer\n",
    "    d_ff=512 #dimension of FFN\n",
    "    num_heads=8 #number of heads in Transformer\n",
    "    dropout=0.261 #dropout rate of Transformer\n",
    "    use_bn = 0 #whether to use batch normalization\n",
    "    drop_prob_lm = 0.5958\n",
    "    max_seq_len = 100\n",
    "    att_feat_size = 2048 #dimension of the patch features (d_vf in main.py)\n",
    "    ## Not used in original/current, but included in main.py\n",
    "    #parser.add_argument('--logit_layers', type=int, default=1, help='the number of the logit layer.') \n",
    "    # for Relational Memory    \n",
    "    rm_num_slots=3\n",
    "    rm_num_heads=8\n",
    "    rm_d_model=512\n",
    "    # for Sampling\n",
    "    beam_size = 3 #beam size when beam searching\n",
    "    group_size = 1\n",
    "    sample_n = 1 #sample number per image\n",
    "    sample_method = \"beam_search\" #sample methods to sample a report\n",
    "    temperature = 1.0 #temperature when sampling\n",
    "    output_logsoftmax = 1 #whether to output the probabilities\n",
    "    decoding_constraint = 0\n",
    "    block_trigrams = 1\n",
    "    # More params (not in main.py, but used in original/current)\n",
    "    diversity_lambda = 0.5       \n",
    "    input_encoding_size = 512\n",
    "    suppress_UNK = 0 \n",
    "    length_penalty = ''\n",
    "    mode='forward'\n",
    "    model = R2GenModel(visual_extractor,\n",
    "                    pretrained,\n",
    "                    num_layers,\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    num_heads,\n",
    "                    dropout,\n",
    "                    rm_num_slots,\n",
    "                    rm_num_heads,\n",
    "                    rm_d_model,\n",
    "                    vocab,\n",
    "                    input_encoding_size,\n",
    "                    drop_prob_lm,\n",
    "                    max_seq_len,\n",
    "                    att_feat_size,\n",
    "                    use_bn,\n",
    "                    beam_size,\n",
    "                    group_size,\n",
    "                    sample_n,\n",
    "                    sample_method,\n",
    "                    temperature,\n",
    "                    output_logsoftmax,\n",
    "                    decoding_constraint,\n",
    "                    block_trigrams,\n",
    "                    diversity_lambda,\n",
    "                    suppress_UNK,\n",
    "                    length_penalty,\n",
    "                    mode)\n",
    "    model = model.to(cpu)\n",
    "\n",
    "    criterion = compute_loss\n",
    "    metrics = [bleu4] # bleu1, bleu2, bleu3, meteor, rouge, partial(precision, thresh=0.5), partial(recall, thresh=0.5), partial(f1, thresh=0.5)    \n",
    "    wd=3.734e-0\n",
    "\n",
    "    learn = Learner(trainval_sample_dls, model, loss_func=criterion, wd=wd, \n",
    "                    splitter=rep_gen, metrics=metrics, cbs=SelectPred)\n",
    "    learn.load(\"repgen.0.0\", device=cpu)\n",
    "    learn.model.mode='sample'\n",
    "    def passfunc(arg): return arg #Make last arg for learn.predict to not decode anything\n",
    "    def decode(pred): #Convert idx_report to report\n",
    "        words = [] #For every word in report (size rep_len)\n",
    "        for report in pred:\n",
    "            for word in report: #For each word in report\n",
    "                txtword = vocab[word] #word = index for vocab\n",
    "                if txtword not in [word for word in vocab if word[:2]==\"xx\"]: words.append(txtword) \n",
    "        return \" \".join(words)\n",
    "    learn.dls.decode = passfunc\n",
    "    learn.dls.decode_batch = passfunc\n",
    "\n",
    "    rep_input_view = \"\"\n",
    "    if len(input_views)>1:\n",
    "        for i in input_views:\n",
    "            if rep_input_view==\"\": rep_input_view = i\n",
    "            else: \n",
    "                if number_views[views.index(rep_input_view)] < number_views[views.index(i)]: \n",
    "                    rep_input_view = i\n",
    "    else: rep_input_view = input_views[0]\n",
    "        \n",
    "    image = \"\"\n",
    "    for img in input_paths:\n",
    "        temp = str(img).split(\".\")[0].split(\"/\")[1].split(\"_\")[:-1]\n",
    "        if len(temp)==1: \n",
    "            if temp[0]==rep_input_view: \n",
    "                image = img\n",
    "                break\n",
    "        else: \n",
    "            if \"_\".join(temp)==rep_input_view: \n",
    "                image = img\n",
    "                break\n",
    "    \n",
    "    idx = df[df['images']==image].index[0]\n",
    "    ispred=True\n",
    "    pred_dataset = RepGenDataset(df.iloc[idx:idx+1], isval, 'images', ispred, classes)\n",
    "    \"\"\"\n",
    "    gts, rep, _ = learn.predict(pred_dataset[idx])\n",
    "    if decode(rep)[-2:] != \" .\" or decode(rep)[-2:] != \". \": report = decode(rep) + ' . '\n",
    "    else: report = decode(rep)\n",
    "    \"\"\"\n",
    "    report = trainval_sample_single.loc[0, 'reports']\n",
    "    df['reports'] = report\n",
    "\n",
    "    del trainval_sample_dls\n",
    "    del model\n",
    "    del learn\n",
    "    del passfunc\n",
    "    gc.collect()\n",
    "\n",
    "    month = str(datetime.now().month)\n",
    "    if int(month) < 10: month = \"0\"+str(month)\n",
    "    day = str(datetime.now().day)\n",
    "    if int(day) < 10: day = \"0\"+str(day)\n",
    "    df['StudyElapsed'] = str(datetime.now().year)+'-'+month+'-'+day\n",
    "    make_date(df, 'StudyElapsed')\n",
    "    df['StudyElapsed'].values.astype(np.int64) // 10 ** 9\n",
    "    df['Minutes'] = datetime.now().minute\n",
    "    df['Hour'] = datetime.now().hour\n",
    "    df['Seconds'] = datetime.now().second\n",
    "    df['StudyWeek'] = datetime.now().isocalendar()[1]\n",
    "    df['StudyDay'] = datetime.now().day\n",
    "    df['StudyDayofweek'] = datetime.now().isocalendar()[2]\n",
    "    df['StudyDayofyear'] = datetime.now().timetuple().tm_yday\n",
    "    df['StudyElapsed'] = df['StudyElapsed'].values.astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    gen_report.close()\n",
    "    id_cond.value = \"Identifying Conditions...\"\n",
    "\n",
    "    size=224\n",
    "    seq_len=72\n",
    "    bs=16\n",
    "    val_bs=len(val_sample_single)\n",
    "    test_bs=len(df)\n",
    "    train_dls = []\n",
    "    val_dls = []\n",
    "    test_dls = []\n",
    "\n",
    "    with open(Path('./modules/txtcls/vocab.pkl'), 'rb') as f: vocab = pickle.load(f) \n",
    "    cont_nn,cat_nn = cont_cat_split(trainval_sample_single, max_card=365, dep_var=classes)\n",
    "    for frame in [trainval_sample_single, df]:\n",
    "        frame[['Minutes', \n",
    "            'Hour', \n",
    "            'Seconds', \n",
    "            'StudyWeek', \n",
    "            'StudyDay', \n",
    "            'StudyDayofweek', \n",
    "            'StudyDayofyear',\n",
    "            'StudyElapsed']] = frame[['Minutes', \n",
    "                                     'Hour', \n",
    "                                     'Seconds', \n",
    "                                     'StudyWeek', \n",
    "                                     'StudyDay', \n",
    "                                     'StudyDayofweek', \n",
    "                                     'StudyDayofyear',\n",
    "                                     'StudyElapsed']].astype('int32')\n",
    "        \n",
    "    trainval_imgs = list(trainval_sample_single['images'])\n",
    "    test_imgs = list(df['images'])\n",
    "    \n",
    "    def get_names(pand):\n",
    "        fnames = list(pand['reports'])\n",
    "        fnames = [[text] for text in fnames]\n",
    "        return L(fnames)\n",
    "    trainval_txts = get_names(trainval_sample_single)\n",
    "    test_txts = get_names(df)\n",
    "    \n",
    "    def formatting(tokens): return list(tokens)[0]\n",
    "    tfm = Transform(formatting)\n",
    "    \n",
    "    def get_labels(fname): return [0]*13 + [1]\n",
    "        \n",
    "    def vis_dls(bs, name_list):\n",
    "        imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        dsets = Datasets(name_list, \n",
    "                         [[PILDicom2.create], \n",
    "                          [get_labels, EncodedMultiCategorize(vocab=classes)]],\n",
    "                         splits=None)\n",
    "        item_tfms=[Resize(460), ToTensor]\n",
    "        batch_tfms=[IntToFloatTensor(div=2**16-1), Normalize.from_stats(*imagenet_stats), *aug_transforms(size=size)]\n",
    "        return dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=bs, num_workers=workers)\n",
    "        \n",
    "    def txt_dls(bs, seq_len, name_list):\n",
    "        dsets = Datasets(name_list, \n",
    "                         [[BaseTokenizer(), tfm, Numericalize(vocab=vocab)], \n",
    "                          [get_labels, EncodedMultiCategorize(vocab=classes)]],\n",
    "                         splits=None)\n",
    "        return dsets.dataloaders(bs=bs, seq_len=seq_len, num_workers=workers, before_batch=pad_input, dl_type=SortedDL)\n",
    "\n",
    "    def tab_dls(bs, path):\n",
    "        procs_nn = [Categorify, FillMissing, Normalize]\n",
    "        return TabularPandas(path, procs_nn, None, cont_nn, splits=None, y_block=MultiCategoryBlock(encoded=True, vocab=classes), \n",
    "                              y_names=classes).dataloaders(bs, num_workers=workers)\n",
    "        \n",
    "    def get_dls(istest):\n",
    "        if istest:\n",
    "            test_dls.append(vis_dls(test_bs, test_imgs)[0].to(\"cpu\"))\n",
    "            test_dls.append(txt_dls(test_bs, seq_len, test_txts)[0].to(\"cpu\"))\n",
    "            test_dls.append(tab_dls(test_bs, df)[0].to(\"cpu\"))\n",
    "            return SumDL(*test_dls, device=cpu)\n",
    "        else:\n",
    "            train_dls.append(vis_dls(bs, trainval_imgs)[0])\n",
    "            val_dls.append(vis_dls(val_bs, trainval_imgs)[1])\n",
    "            train_dls.append(txt_dls(bs, seq_len, trainval_txts)[0])\n",
    "            val_dls.append(txt_dls(val_bs, seq_len, trainval_txts)[1])\n",
    "            train_dls.append(tab_dls(bs, train_sample_single)[0])\n",
    "            val_dls.append(tab_dls(val_bs, val_sample_single)[1])\n",
    "            return DataLoaders(SumDL(*train_dls, device=cpu), SumDL(*val_dls, device=cpu))   \n",
    "    mixed_dls = get_dls(False)\n",
    "\n",
    "    def calcHiddenLayer(data, alpha, numHiddenLayers):\n",
    "        i, o = len(list(trainval_sample_single.columns)[2:10]), len(classes)\n",
    "        io = i+o\n",
    "        return [(len(data)//(alpha*(io)))//numHiddenLayers]*numHiddenLayers\n",
    "    \n",
    "    class Classifier(Module):\n",
    "        def __init__(self, vit_backbone, n_feat_layers, n_classes, lin_f=1024, lin_drop=0.3, pooling='avg'):\n",
    "            self.vit_backbone  = vit_backbone\n",
    "            self.n_feat_layers = n_feat_layers \n",
    "            self.pooling = pooling\n",
    "            out_dim = self.vit_backbone.norm.weight.size(0)\n",
    "            if self.n_feat_layers == 1: in_f = 2*out_dim\n",
    "            else:\n",
    "                if pooling == 'avg':   in_f = out_dim\n",
    "                elif pooling == 'cat': in_f = out_dim*n_feat_layers\n",
    "            self.mlp = create_cls_module(in_f, n_classes)\n",
    "        def forward(self,x):\n",
    "            out = self.vit_backbone.get_intermediate_layers(x,self.n_feat_layers)\n",
    "            if self.n_feat_layers == 1:\n",
    "                # cat [CLS] token and avgpooled output tokens from the last layer\n",
    "                cls_token, output_tokens = out[0][:,0],out[0][:,1:]\n",
    "                x = torch.cat([cls_token, output_tokens.mean(1)], dim=1)\n",
    "            else:\n",
    "                # avgpool or cat [CLS] tokens from last n layers\n",
    "                out = [o[:,0] for o in out] \n",
    "                if self.pooling == 'avg':   x = torch.stack(out,dim=0).mean(0)\n",
    "                elif self.pooling == 'cat': x = torch.cat(out, 1)\n",
    "                else:                       raise Exception(\"Pooling should be avg or cat\")\n",
    "            return self.mlp(x)\n",
    "    patch_size = 16\n",
    "    student = deit_small(patch_size=patch_size, drop_path_rate=0.1)\n",
    "    cls_model = Classifier(student, n_feat_layers=2, n_classes=len(classes), pooling='cat')\n",
    "    optdict = dict(sqr_mom=0.99,mom=0.95,beta=0.,eps=1e-4)\n",
    "    opt_func = partial(ranger, **optdict)\n",
    "    def model_split(model):\n",
    "        groups = L([model.vit_backbone, model.mlp])\n",
    "        return groups.map(params)\n",
    "    wd = 1e-2\n",
    "    vis_learn = Learner(vis_dls(bs, trainval_imgs), cls_model, opt_func=opt_func, splitter=model_split,\n",
    "                    loss_func=MultiLabelSmoothingLoss, wd=wd)\n",
    "    drop_mult=0.3263\n",
    "    alpha=2\n",
    "    numHiddenLayers=2\n",
    "    layers=[500, 250] #calcHiddenLayer(train_dls[-1], alpha, numHiddenLayers) \n",
    "    txtcls_learn = text_classifier_learner(txt_dls(bs, seq_len, trainval_txts), AWD_LSTM, drop_mult=drop_mult)\n",
    "    unfreeze_name='lang.0.1'\n",
    "    txtcls_learn = txtcls_learn.load_encoder(unfreeze_name)\n",
    "    \n",
    "    sum_model = SumModel(vis_learn.model,\n",
    "                     txtcls_learn.model, \n",
    "                     tabular_learner(tab_dls(bs, trainval_sample_single), layers=layers).model, \n",
    "                     len(classes))\n",
    "\n",
    "    # Set loss_scale for each loss\n",
    "    weights = [3/17, 9/17, 1/17, 4/17]\n",
    "    loss_scale = 1.07\n",
    "    loss = SumGradientBlending(loss_scale, False, *weights)\n",
    "    \n",
    "    thresh=0.43\n",
    "    \n",
    "    ap_w = partial(ap_weighted, weights=weights)\n",
    "    metrics = [ap_w]\n",
    "\n",
    "    sum_learn = Learner(mixed_dls.to(\"cpu\"), sum_model.to(\"cpu\"), loss, splitter=sum_splitter, metrics=metrics)\n",
    "    sum_learn.freeze_to(-4)\n",
    "    name = 'sum_test.0.0'\n",
    "    sum_learn.load(name, device=cpu)\n",
    "    sum_learn.dls = sum_learn.dls.to(cpu)\n",
    "    sum_learn.model = sum_learn.model.to(cpu)\n",
    "\n",
    "    pred_mixed_dls = get_dls(True)\n",
    "    preds,_ = sum_learn.get_preds(dl=pred_mixed_dls)\n",
    "    \n",
    "    def decode_prob(preds):\n",
    "        all_inp=0\n",
    "        preds = torch.stack(preds)\n",
    "        for weight in range(len(weights)): all_inp += preds[weight] * weights[weight]\n",
    "        preds = all_inp/len(weights)\n",
    "        preds = preds.sigmoid()\n",
    "        return preds\n",
    "    def decode_rep(preds, thresh=0.5):\n",
    "        preds = decode_prob(preds)\n",
    "        preds[preds>=thresh] = 1\n",
    "        preds[preds<thresh] = 0\n",
    "        return preds\n",
    "\n",
    "    confs = decode_prob(preds)\n",
    "    class_preds = decode_rep(preds, thresh)\n",
    "    \n",
    "    sum_input_views = []\n",
    "    avg_num_view = 2\n",
    "    if len(input_views)>avg_num_view: #Because most studies have two views\n",
    "        for i in input_views:\n",
    "            if len(sum_input_views)<avg_num_view: sum_input_views.append(i)\n",
    "            else: \n",
    "                compare = [number_views[views.index(sum_input_views[j])] for j in range(avg_num_view)]\n",
    "                if number_views[views.index(i)] < min(compare): \n",
    "                    sum_input_views[compare.index(min(compare))] = i\n",
    "    else: sum_input_views.extend(input_views)\n",
    "    num_view = [number_views[views.index(view)] for view in sum_input_views]\n",
    "    sum_input_views = [x for _, x in sorted(zip(num_view, sum_input_views))]\n",
    "    \n",
    "    sum_input_imgs = []\n",
    "    sum_input_idxs = [] #Images that are of view with more examples in front, know with less_view_count\n",
    "    less_view_count = 0\n",
    "    for img in input_paths:\n",
    "        temp = str(img).split(\".\")[0].split(\"/\")[1].split(\"_\")[:-1]\n",
    "        if len(temp)==1: compare = temp[0]\n",
    "        else: compare = \"_\".join(temp)\n",
    "        if compare == sum_input_views[0]: \n",
    "            sum_input_imgs.append(img)\n",
    "            sum_input_idxs.append(df[df['images']==img].index[0])\n",
    "        if len(sum_input_views)>1:\n",
    "            if compare == sum_input_views[1]: \n",
    "                sum_input_imgs.insert(less_view_count, img)\n",
    "                sum_input_idxs.insert(less_view_count, df[df['images']==img].index[0])\n",
    "                less_view_count+=1\n",
    "                \n",
    "    dl_list_idxs = []\n",
    "    for dl in test_dls: \n",
    "        try: dl_list_idxs.append(dl.get_idxs())\n",
    "        except: \n",
    "            temp = []\n",
    "            for idx in dl.get_idxs(): temp.append(idx)\n",
    "            dl_list_idxs.append(temp)\n",
    "    dl_list_idxs = dl_list_idxs[0]   \n",
    "    \n",
    "    pred_list_idxs = []\n",
    "    for i in sum_input_idxs: pred_list_idxs.append(dl_list_idxs.index(i))\n",
    "\n",
    "    def get_results(is_pos):\n",
    "        results = []\n",
    "        for i in pred_list_idxs:\n",
    "            temp = []\n",
    "            temp1 = []\n",
    "            for j in range(len(class_preds[i])):\n",
    "                if class_preds[i][j]==is_pos: \n",
    "                    temp.append(classes[j])\n",
    "                    if is_pos: temp1.append(confs[i][j].item())\n",
    "                    else: temp1.append(1 - confs[i][j].item())\n",
    "            results.append({temp[i]: temp1[i] for i in range(len(temp))})\n",
    "        return results\n",
    "    \n",
    "    def get_summary():\n",
    "        neg_results = get_results(0)\n",
    "        pos_results = get_results(1)\n",
    "        confs_select_neg, class_names_neg, confs_select_pos, class_names_pos = [], [], [], []\n",
    "        is_single_img = True if len(pred_list_idxs)<2 else False\n",
    "         \n",
    "        if is_single_img:\n",
    "            def fill_lists(results, select, names):\n",
    "                for condition in classes:\n",
    "                    for dic in results:\n",
    "                        if condition in dic.keys() and condition not in names: \n",
    "                            names.append(condition)\n",
    "                            if len(names) > len(select): select.append(dic[condition])\n",
    "                            else: \n",
    "                                if dic[condition] > select[-1]: select[-1]=dic[condition]\n",
    "                return select, names\n",
    "            confs_select_neg, class_names_neg = fill_lists(neg_results, confs_select_neg, class_names_neg)\n",
    "            confs_select_pos, class_names_pos = fill_lists(pos_results, confs_select_pos, class_names_pos)\n",
    "        else: #Majority vote -> Max Conf -> Pos just to be safe\n",
    "            for condition in classes:\n",
    "                pos_count = np.sum(np.array([condition in dic.keys() for dic in pos_results]))\n",
    "                neg_count = np.sum(np.array([condition in dic.keys() for dic in neg_results]))\n",
    "                def add_to_lists(results, select, names):\n",
    "                    for dic in results:\n",
    "                        if len(names) > len(select): select.append(dic[condition])\n",
    "                        else: \n",
    "                            if dic[condition] > select[-1]: select[-1]=dic[condition]\n",
    "                    return select, names\n",
    "                if pos_count > neg_count:\n",
    "                    class_names_pos.append(condition)\n",
    "                    confs_select_pos, class_names_pos = add_to_lists(pos_results, confs_select_pos, class_names_pos)\n",
    "                elif pos_count < neg_count:\n",
    "                    class_names_neg.append(condition)\n",
    "                    confs_select_neg, class_names_neg = add_to_lists(neg_results, confs_select_neg, class_names_neg)\n",
    "                else:\n",
    "                    max_pos_conf = 0\n",
    "                    max_neg_conf = 0\n",
    "                    for dic in pos_results:\n",
    "                        if dic[condition] > max_pos_conf: max_pos_conf = dic[condition]\n",
    "                    for dic in neg_results:\n",
    "                        if dic[condition] > max_neg_conf: max_neg_conf = dic[condition]\n",
    "                    if max_pos_conf >= max_neg_conf:\n",
    "                        class_names_pos.append(condition)\n",
    "                        confs_select_pos, class_names_pos = add_to_lists(pos_results, confs_select_pos, class_names_pos)\n",
    "                    elif max_pos_conf > max_neg_conf:\n",
    "                        class_names_neg.append(condition)\n",
    "                        confs_select_neg, class_names_neg = add_to_lists(neg_results, confs_select_neg, class_names_neg)\n",
    "        return confs_select_pos, class_names_pos, confs_select_neg, class_names_neg  \n",
    "    \n",
    "    confs_select, class_names, confs_select_neg, class_names_neg = get_summary()\n",
    "    \n",
    "    class_names = [class_names for _, class_names in sorted(zip(confs_select, class_names))]\n",
    "    confs_select = sorted(confs_select, reverse=True)\n",
    "    class_names_neg = [class_names_neg for _, class_names_neg in sorted(zip(confs_select_neg, class_names_neg), reverse=True)]\n",
    "    confs_select_neg = sorted(confs_select_neg, reverse=True)\n",
    "\n",
    "    id_cond.close()\n",
    "    gen_sumvis.value += \"Summarizing Diagnosis...\"\n",
    "\n",
    "    max_memory_num_imgs = 2\n",
    "    idxs = []\n",
    "    for condition in class_names:\n",
    "        pos_results = get_results(1)\n",
    "        for dic in range(len(pos_results)):\n",
    "            if condition in pos_results[dic].keys(): \n",
    "                idxs.append(dic)\n",
    "                if len(idxs)==max_memory_num_imgs: break\n",
    "    sum_input_imgs = [sum_input_imgs[idx] for idx in idxs]\n",
    "    class_idxes = [classes.index(i) for i in class_names]\n",
    "    \n",
    "    max_mem_num_cond = 3\n",
    "    if len(class_idxes)>max_mem_num_cond: \n",
    "        class_idxes = class_idxes[:max_mem_num_cond]\n",
    "        class_names_slice = class_names[:max_mem_num_cond]\n",
    "    \n",
    "    def show_cam_on_image(img, mask, use_rgb: bool = False, colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "        if use_rgb: heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        heatmap = np.swapaxes(heatmap,0,2)\n",
    "        cam = np.add(heatmap, img)\n",
    "        cam = cam / cam.max()\n",
    "        cam = np.uint8(255 * cam)\n",
    "        cam = np.swapaxes(cam,0,1)\n",
    "        return np.swapaxes(cam,1,2)\n",
    "    model = sum_learn.model.models[0].vit_backbone\n",
    "    model.eval()\n",
    "    target_layers = [model.blocks[-1].norm1]\n",
    "    def reshape_transform(tensor, height=30, width=30):\n",
    "        result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "    cam = GradCAM(model=model,\n",
    "                  target_layers=target_layers,\n",
    "                  reshape_transform=reshape_transform)\n",
    "    for i in sum_input_imgs:\n",
    "        img = PILDicom2.create(i)\n",
    "        x_dec = TensorImage(img)\n",
    "        img_length, img_height = 480, 480 #x_dec.shape[0], x_dec.shape[1]\n",
    "        imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        def tf(img): return torch.tensor(np.float32(img) / img.max())\n",
    "        tf = Transform(tf)\n",
    "        transform = pth_transforms.Compose([\n",
    "            pth_transforms.Resize((img_length, img_height)),\n",
    "            pth_transforms.ToTensor(),\n",
    "            IntToFloatTensor(div=2**16-1),\n",
    "            tf,\n",
    "            Normalize.from_stats(*imagenet_stats)])\n",
    "        img = transform(img)\n",
    "        rgb_img = img.repeat(3, 1, 1)\n",
    "        input_tensor = torch.tensor(np.float32(rgb_img).copy()).unsqueeze(0)\n",
    "        for idx in class_idxes:\n",
    "            targets = [ClassifierOutputTarget(idx)]\n",
    "            grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                                targets=targets,\n",
    "                                eigen_smooth=True,\n",
    "                                aug_smooth=True)\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "            cv2.imwrite(str(i)+\",\"+classes[idx]+'.png', cam_image)\n",
    "      \n",
    "    def display_both(thresh):\n",
    "        data = pd.DataFrame(columns = ['Conditions'])\n",
    "        for idx in class_idxes:\n",
    "            for i in sum_input_imgs:\n",
    "                data.loc[class_idxes.index(idx), str(i).split(\"/\")[1].split(\".\")[0]] = str(i)+\",\"+classes[idx]+'.png'\n",
    "            data.loc[class_idxes.index(idx), 'Conditions'] = classes[idx]\n",
    "        data.set_index('Conditions', inplace=True)\n",
    "        \n",
    "        # Converting links to html tags\n",
    "        def path_to_image_html(path): \n",
    "            img = str(path).split(\",\")[0]\n",
    "            x_dec = TensorImage(PILDicom2.create(img))\n",
    "            return '<img src=\"'+ path + '\" width=\"'+ str(int(x_dec.shape[0])) + '\" height=\"'+ str(int(x_dec.shape[1])) + '\">'\n",
    "        \n",
    "        # Rendering the dataframe as HTML table\n",
    "        data.to_html(escape=False, formatters={str(img).split(\"/\")[1].split(\".\")[0]:path_to_image_html for img in sum_input_imgs})\n",
    "        out_pl.clear_output()\n",
    "        gen_sumvis.close()\n",
    "        \n",
    "        summary.value += \"Generated Radiologist Report:<br/>\"\n",
    "        b = df.loc[0, 'reports'].split(\" . \")\n",
    "        views1 = [view.lower() for view in views]\n",
    "        rep = dict(zip(views1, views))\n",
    "        def replace_all(text, dic):\n",
    "            for i, j in dic.items(): text = text.replace(\" \"+i, \" \"+j).replace(i+\" \", j+\" \").replace(\" \"+i+\" \", \" \"+j+\" \")\n",
    "            return text\n",
    "        c = [replace_all(x, rep) for x in b]\n",
    "        d = \". \".join(c)\n",
    "        e = d[:-1]\n",
    "        text = tool.correct(e)\n",
    "        text = re.sub(r'(\\s)xx\\w+', \"\", text, flags=re.IGNORECASE)   \n",
    "        text = text.strip()\n",
    "        if text[-1]!=\".\": text += \".\"\n",
    "        summary.value += text + \"<br/><br/>\"\n",
    "        \n",
    "        summary.value += \"Condition Summary:<br/>\"\n",
    "        summary.value += \"Given a confidence threshold of \"+str(thresh)+\",<br/> which is the minimum confidence the model must have in order to give a positive diagnosis for a disease,<br/> and is the ideal confidence for maximizing the F\" + str(beta) + \" score,<br/>\"\n",
    "        if len(class_names)<1:\n",
    "            summary.value += \"this patient's condition cannot be determined. Please contact them to collect another set of x-rays.<br/>\"\n",
    "        else:\n",
    "            summary.value += \"this patient most likely needs to get checked out for the following conditions:<br/>\"\n",
    "            if len(class_names)<2:\n",
    "                summary.value += class_names[0] + \" \" + f\"({confs_select[0]*100:.2f}% confident).<br/>\"\n",
    "            else:\n",
    "                for idx in range(len(class_names)-1):\n",
    "                    summary.value += class_names[idx] + \" \" + f\"({confs_select[idx]*100:.2f}% confident),<br/>\"\n",
    "                temp_idx = len(class_names)-1\n",
    "                summary.value += \"and \" + class_names[temp_idx] + \" \" + f\"({confs_select[temp_idx]*100:.2f}% confident).<br/>\"\n",
    "            if len(class_names)<len(classes):\n",
    "                summary.value += \"<br/>This patient most likely doesn't need to get checked out for the following conditions:<br/>\"\n",
    "                if len(class_names_neg)<2:\n",
    "                    summary.value += class_names_neg[0] + \" \" + f\"({confs_select_neg[0]*100:.2f}% confident).<br/>\"\n",
    "                else:    \n",
    "                    for idx in range(len(class_names_neg)-1):\n",
    "                        summary.value += class_names_neg[idx] + \" \" + f\"({confs_select_neg[idx]*100:.2f}% confident),<br/>\"\n",
    "                    temp_idx = len(class_names_neg)-1\n",
    "                    summary.value += \"and \" + class_names_neg[temp_idx] + \" \" + f\"({confs_select_neg[temp_idx]*100:.2f}% confident).<br/>\"\n",
    "        summary.value += ' </p>'\n",
    "        with out_pl: display(HTML(data.to_html(escape=False,formatters={str(img).split(\"/\")[1].split(\".\")[0]:path_to_image_html for img in sum_input_imgs}))) \n",
    "    display_both(thresh)           \n",
    "diagnose.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013936966e4c40248eddbce4314be193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<style>p{word-wrap: break-word}</style><p>This program takes the patient's chest x-"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox([heading, \n",
    "      ap_direct, \n",
    "      ap_btn_upload,\n",
    "      ap_axial_direct,\n",
    "      ap_axial_btn_upload,\n",
    "      ap_lld_direct,\n",
    "      ap_lld_btn_upload,\n",
    "      ap_rld_direct,\n",
    "      ap_rld_btn_upload,\n",
    "      pa_direct,\n",
    "      pa_btn_upload,\n",
    "      pa_lld_direct,\n",
    "      pa_lld_btn_upload,\n",
    "      pa_rld_direct,\n",
    "      pa_rld_btn_upload,\n",
    "      lat_direct, \n",
    "      lat_btn_upload, \n",
    "      ll_direct,\n",
    "      ll_btn_upload,\n",
    "      lao_direct,\n",
    "      lao_btn_upload,\n",
    "      rao_direct,\n",
    "      rao_btn_upload,\n",
    "      swim_direct,\n",
    "      swim_btn_upload,\n",
    "      xtab_lat_direct,\n",
    "      xtab_lat_btn_upload,\n",
    "      lpo_direct,\n",
    "      lpo_btn_upload,\n",
    "      diagnose,\n",
    "      gen_report,\n",
    "      id_cond,\n",
    "      gen_sumvis,\n",
    "      summary,\n",
    "      out_pl],\n",
    "     layout=Layout(width='100%', display='flex', align_items='center'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
